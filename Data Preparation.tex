\subsection{Data Description}
The 3.4 million rows in the original dataset represent the top 200 songs played every day from January 2017 to January 2018 in 54 different regions of the world. The track names, artist names, region codes (represented in ISO alpha-2 format), and the date of streaming are the attributes in the dataset. Of all instances, 657 lack track and artist names, while an additional 8 lack their unique track URLs. For the remaining 649 songs, we attempt to hit the Spotify API to add the missing artist and track names, but the API response is empty. As a result, we drop all entries with null values.

\subsection{Data Extension}
Since our primary task is to cluster acoustically similar songs based on their intrinsic features, the data available to us is insufficient for us to accomplish our goal. We ignore the region information and focus on extracting the distinct URLs from the dataset, which would provide us with insights into the complete list of tracks streamed during the year across all accessible regions of the world. We use this filter because it is possible for multiple songs with the same name to be produced by various artists, which would lead to loss of essential information. 

The dataset contains 21735 unique URLs. The audio elements of the tracks are used for clustering and accessed from Spotify's database via their developer API. We collapse the dataset to make a new table with the track names, their artists' names, and corresponding URLs. We access the 18 newly fetched audio features of the songs using the URL and add them to the freshly created tabular data. Only 13 of the 18 features are used for clustering. Some of these attributes include measures of the songs' danceability, energy, tempo, valence, instrumentalness etc. A complete list of these features along with their descriptions can be looked up here \cite{spotifyaudiofeatures}. The remaining are extraneous variables like the audio analysis URLs and the track's encoded IDs.

The dataset's final dimensions, which are necessary for the task, are 21735 x 13. To guarantee that all features would contribute equally to the clustering methods used later on, the data is pre-processed by normalising each feature by scaling it down to a range of [0,1]. The MinMaxScalar() method from the \textit{scikit-learn} library is used to implement the normalisation.