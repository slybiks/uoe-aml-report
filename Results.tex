The CH, DB, and SS scores given in Table \ref{tab:results} are the results of several clustering algorithms using various dimensionality reduction techniques. These are evaluated against a baseline model that makes use of the 13 scaled audio properties and greatly surpasses it. These searches lead to the conclusion that the K-Means with PaCMAP is the best model, with the CH, DB, and SS scores coming in at 86258.97, 0.792, and 0.431, respectively. From the overall viewpoint of the clustering results, we discover that DBSCAN is ineffective because many of the data points are labelled as noise or outliers and given the value -1. The points are closely packed although the number of clusters itself is small in cases where the silhouette scores are $\approx0.3$. On the other hand, BIRCH returns the number of sub-clusters, in some cases, more than 100, with only a few song samples belonging to a given class. K-Means has a fair balance in this area between the number of clusters and the goodness of the clustering. The original un-scaled dataset with 13 audio features has a column added with the K-Means clustering labels generated by PaCMAP. We do not know what these clusters represent, even though our experiments have thus far discussed the approaches used for clustering. For each of the cluster labels, we draw a univariate analysis (a grid-map of histograms) for the 13 audio attributes to determine this. The findings and the visualisations are covered in Appendix \ref{appendix:D}. Since the clustering is done for the tracks rather than the artists, One artist could have appeared in more than one cluster because they may have created songs belonging to different genres. We also keep track of how many times each artist is included in a particular cluster. Appendix \ref{appendix:E} contains an extract for this outcome with 30 randomly sampled artists. Using the data from this table as an added feature to cluster artists together is one of the ideas for future implementations.

\begin{table}[hbt]
    \centering
    \rowcolors{1}{white}{gray!25}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \toprule
     \makecell{Clustering \\ Algorithm} & \makecell{Dimensionality \\ Reduction}  & \makecell{No. Clusters} & \makecell{No. Features} & 
     \makecell{CH} & \makecell{DB} & \makecell{SS} \\
    \midrule
        K-Means  & None (Baseline)      &  11      & 13     & 4987.08    & 1.796   & 0.163\\
        K-Means                   & PCA     &  11      & 7    & 6462.19   & 1.58  & 0.194\\
        K-Means                   & UMAP     &  9      & 2    & 97420.38   & 0.875  & 0.406\\
        K-Means                   & Gaussian Random Projection     &  10      & 2    & 20061.29  & 0.867  & 0.338\\
        \textbf{K-Means}                  & \textbf{PaCMAP}    &  \textbf{8}      & \textbf{2}    & \textbf{86258.97}   & \textbf{0.792}  & \textbf{0.431}\\
        K-Means                   & Autoencoder     &  13      & 6    & 16738.36   & 1.496  & 0.186\\
    \midrule
        BIRCH                   & None (Baseline)      &  61      & 13     & 1138.03    & 1.56   & 0.131\\
        BIRCH                   & PCA     &  13      & 7    & 5258.34   & 1.58 & 0.199\\
        BIRCH                   & UMAP     &  180      & 2    & 43173.12   & 0.880  & 0.330\\
        BIRCH                   & Gaussian Random Projection     &  5      & 2    & 20726.05 & 0.919  & 0.353\\
        BIRCH                   & PaCMAP     &  260      & 2    & 96680.87  & 0.864  & 0.324\\
        BIRCH                   & Autoencoder     &  8      & 6    & 66726.78  & 1.089  & 0.331\\
    \midrule
        DBSCAN                   & None (Baseline)      &  5      & 13     & 3522.65    & 1.585   & 0.365\\
        DBSCAN                   & PCA     &  5     & 7    & 3706.69   & 1.829 & 0.191\\
        DBSCAN                   & UMAP     &  5      & 2    & 30712.09   & 1.899  & 0.161\\
        DBSCAN                   & Gaussian Random Projection     &  7      & 2    & 11.501 & 1.632  & 0.168\\
        DBSCAN                   & PaCMAP     &  5      & 2    & 26649.71  & 0.628  & 0.335\\
        DBSCAN                   & Autoencoder     &  3      & 6    & 36465.97  & 0.761  & 0.349\\
    \bottomrule
    \end{tabular}%
    }
    \caption{Experiment results for different combinations of clustering algorithms and dimensionality reduction methods. SS is silhouette score, DB is Daviesâ€“Bouldin index, CH is Calinski Harabasz score. Row in \textbf{bold} indicates the best model.}
    \label{tab:results}
\end{table} 